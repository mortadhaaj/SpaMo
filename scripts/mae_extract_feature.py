import os
import pandas as pd
import numpy as np
import torch
import argparse
import tqdm
import os.path as osp
from PIL import Image
from transformers import VideoMAEModel, VideoMAEImageProcessor

import sys
sys.path.append('./')

from utils.helpers import sliding_window_for_list, read_video, get_img_list

_GLOBAL_SEED = 0
np.random.seed(_GLOBAL_SEED)
torch.manual_seed(_GLOBAL_SEED)
torch.backends.cudnn.benchmark = True


class VideoMAEFeatureReader(object):
    def __init__(
        self, 
        model_name='MCG-NJU/videomae-large', 
        cache_dir=None,
        device='cuda:0',
        overlap_size=0,
        nth_layer=-1
    ):
        self.device = device
        self.overlap_size = overlap_size
        self.nth_layer = nth_layer

        self.image_processor = VideoMAEImageProcessor.from_pretrained(model_name, cache_dir=cache_dir)
        self.model = VideoMAEModel.from_pretrained(model_name).to(self.device).eval()
        
    @torch.no_grad()
    def get_feats(self, video):
        inputs = self.image_processor(images=video, return_tensors="pt").to(self.device)
        
        outputs = self.model(**inputs, output_hidden_states=True).hidden_states
        
        outputs = outputs[self.nth_layer]
        outputs = outputs[:, 0]
        
        return outputs


def get_parser():
    parser = argparse.ArgumentParser()
    parser.add_argument('--csv_ann_file', help='Isharah Selfie annotation csv file path', required=True)
    parser.add_argument('--anno_root', help='location of tsv files', required=True)
    parser.add_argument('--video_root', help='location of tsv files', required=True)
    parser.add_argument('--save_dir', help='where to save the output', required=True)
    parser.add_argument('--model_name', help='ViT model name', default='MCG-NJU/videomae-large')
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--device', help='device to use', default='cpu')
    parser.add_argument('--overlap_size', type=int, default=8)
    parser.add_argument('--mode', nargs='+', type=str)
    parser.add_argument('--nth_layer', type=int, default=-1)
    parser.add_argument('--cache_dir', help='cache dir for model', default=None)
    return parser


def get_iterator(args, mode, is_isharah=False):
    batch_size = args.batch_size

    if not is_isharah:
        data = np.load(os.path.join(args.anno_root, f'{mode}_info.npy'), allow_pickle=True).item()
        num = len(data) - 1
        
    else:
        df = pd.read_csv(args.csv_ann_file)
        df = df[df.split==mode]
        data = [osp.join(args.video_root, i.rsplit('/', 1)[-1]) for i in df.video_pth.values]
        num = len(data)

    ds_name = osp.split(args.anno_root)[-1]

    reader = VideoMAEFeatureReader(
        args.model_name, 
        device=args.device, 
        overlap_size=args.overlap_size, 
        nth_layer=args.nth_layer,
        cache_dir=args.cache_dir
    )
    
    def iterate():
        for i in range(num):
            if not is_isharah:
                fname = data[i]['folder']
            else:
                fname = data[i]
            
            if ds_name == 'Phoenix14T' or ds_name == 'CSL-Daily':
                image_list = get_img_list(ds_name, args.video_root, fname)
                
                if len(image_list) < 16:
                    len_diff = 16 - len(image_list)
                    image_list.extend([image_list[-1]] * (16 - len(image_list)))
                image_list_chunks = sliding_window_for_list(image_list, window_size=16, overlap_size=args.overlap_size)
                
                videos = []
                for image_list in image_list_chunks:
                    videos.append([Image.open(image).convert('RGB') for image in image_list])
                
                video_feats = []
                for j in range(0, len(videos), batch_size):
                    video_batch = videos[j:min(j + batch_size, len(videos))]
                    feats = reader.get_feats(video_batch).cpu().numpy()
                    video_feats.append(feats)
                    
                yield np.concatenate(video_feats, axis=0), data[i]['fileid'], None
            
            else:
                if ds_name == 'How2Sign':
                    start_time, end_time = data[i]['original_info']['START_REALIGNED'], data[i]['original_info']['END_REALIGNED']
                    videos = read_video(fname, start_time=start_time, end_time=end_time)
                elif ds_name == 'IsharahSelfie':
                    start_time, end_time = None, None
                    videos = read_video(fname, start_time=start_time, end_time=end_time)
                    
                if len(videos) > 0:
                    if len(videos) < 16:
                        len_diff = 16 - len(videos)
                        videos.extend([videos[-1]] * (16 - len(videos)))
                    
                    videos = sliding_window_for_list(videos, window_size=16, overlap_size=args.overlap_size)
                    
                    video_feats = []
                    for j in range(0, len(videos), batch_size):
                        video_batch = videos[j:min(j + batch_size, len(videos))]
                        feats = reader.get_feats(video_batch).cpu().numpy()
                        video_feats.append(feats)
                    
                    if is_isharah:
                        yield np.concatenate(video_feats, axis=0), data[i].rsplit('/', 1)[-1].split('.')[0], str(start_time)
                    else:
                        yield np.concatenate(video_feats, axis=0), data[i]['fileid'], str(start_time)

                else:
                    yield [], data[i]['fileid'], str(start_time)
    
    return iterate, num

def main():
    parser = get_parser()
    args = parser.parse_args()

    mode = ["train"]#, "test", "train"]
    for m in mode:
        ds_name = osp.split(args.anno_root)[-1]
        fname = f'mae_feat_{ds_name}'
        os.makedirs(osp.join(args.save_dir, fname, m), exist_ok=True)
    
        if ds_name == 'How2Sign' or 'IsharahSelfie':
            if m == 'dev': _m = 'val'
            else: _m = m
        elif ds_name == 'NIASL2021':
            if m == 'dev': _m = 'validation' 
        else:
            _m = m

        if ds_name == 'IsharahSelfie':
            generator, num = get_iterator(args, _m, is_isharah=True)
        else:
            generator, num = get_iterator(args, _m)
        iterator = generator()

        for vit_feat in tqdm.tqdm(iterator, total=num):
            feats, id, st = vit_feat
            save_path = osp.join(args.save_dir, fname, m)
            postfix = f'_overlap-{args.overlap_size}'
            
            if st is not None:
                postfix = f'_{st}{postfix}'
            
            np.save(osp.join(save_path, f'{id}{postfix}.npy'), feats)


if __name__ == "__main__":
    main()
